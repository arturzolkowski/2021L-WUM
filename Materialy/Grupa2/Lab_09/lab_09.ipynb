{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasteryzacja\n",
    "#### Czym jest klasteryzacja?\n",
    "Najprościej — szukaniem skupień (klastrów).\n",
    "![](https://miro.medium.com/max/561/0*ff7kw5DRQbs_uixR.jpg)\n",
    "<div align=\"center\">Żródło: https://www.kdnuggets.com/2019/09/hierarchical-clustering.html</div>\n",
    "\n",
    "#### Po co?\n",
    "* Aby znaleźć „naturalne” podziały w zbiorze.\n",
    "* Aby zaproponować podział na klasy.\n",
    "\n",
    "#### Czym właściwie jest klaster?\n",
    "Nie mamy jednej definicji. Na ogół — grupa podobnych obiektów. Różne algorytmy rożnie „rozumieją” podobieństwo i różnie go szukają.\n",
    "\n",
    "Zasadniczo metody dzielimy na metody hierarchiczne i kombinatoryczne. Zacznijmy od kombinatorycznych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizacja danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Jeśli mamy atrybuty o ściśle określonym znaczeniu. Np. LAT LONG, nie powinniśmy skalować danych, ponieważ spowoduje to zniekształcenie.\n",
    "\n",
    "- Jeśli mamy mieszane dane liczbowe, w których każda zmienna jest czymś zupełnie innym (powiedzmy, rozmiar i waga buta), ma dołączone różne jednostki (funty, tony, m, kg ...), to te wartości i tak nie są tak naprawdę porównywalne; Standaryzacja ich jest najlepszą praktyką."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.stack.imgur.com/xAsku.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metoda k-średnich\n",
    "Pomysł jest prosty:\n",
    "1. Zakładamy, że w zbiorze jest k klastrów.\n",
    "2. Wybieramy k punktów będącymi początkowymi położeniami środków naszych klastrów.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/5/5e/K_Means_Example_Step_1.svg)\n",
    "3. Określamy przynależność do klastrów jako przynależność do klastra „generowanego” przez najbliższy środek.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/a/a5/K_Means_Example_Step_2.svg)\n",
    "4. Aktualizujemy położenie środków klastrów. Najczęściej nowym środkiem skupienia jest punkt, którego współrzędne są średnią arytmetyczną współrzędnych punktów należących do danego skupienia.\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/3/3e/K_Means_Example_Step_3.svg)\n",
    "5. Sprawdzamy, czy przynależność jakiegokolwiek punktu zmieniła się po wyznaczeniu nowych środków. Jeśli tak — wracamy do punktu 3. Jeśli nie — kończymy działanie.\n",
    "\n",
    "Źródło ilustracji: https://en.wikipedia.org/wiki/K-means_clustering\n",
    "\n",
    "Działanie algorytmu wygląda następująco:\n",
    "![](https://miro.medium.com/max/960/1*KrcZK0xYgTa4qFrVr0fO2w.gif)\n",
    "<div align=\"center\">Żródło: https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T17:45:00.974632Z",
     "start_time": "2020-04-25T17:45:00.442764Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.9, random_state=314)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T17:45:02.914990Z",
     "start_time": "2020-04-25T17:45:02.835823Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def plot_kmeans_clusters(X, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    y_kmeans = kmeans.predict(X)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=30, cmap='viridis')\n",
    "\n",
    "    centers = kmeans.cluster_centers_\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.75)\n",
    "    plt.title('K-means clusters')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T17:45:03.954158Z",
     "start_time": "2020-04-25T17:45:03.798612Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_kmeans_clusters(X, n_clusters=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A co jeśli podamy złe k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T17:45:19.021674Z",
     "start_time": "2020-04-25T17:45:18.838363Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_kmeans_clusters(X, n_clusters=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zatem jak dobrać liczbę skupień?\n",
    "\n",
    "#### Metoda łokcia\n",
    "Zacznijmy od tego, że algorytm k-średnich minimalizuję wewnątrzklastrową sumę kwadratów (ang. <em>within-cluster sum of squares</em>):\n",
    "\n",
    "$$\\underset{\\mathbf{S}}{\\operatorname{argmin}}\\sum_{i=1}^{k}\\sum_{\\mathbf{x} \\in S_i} {\\lVert \\mathbf{x} - \\mu_i \\rVert^2} = \\underset{\\mathbf{S}}{\\operatorname{argmin}}\\sum_{i=1}^{k} |S_i| \\mathrm{Var} (S_i)\n",
    "$$\n",
    "\n",
    "gdzie $S = \\{S1, S2, ..., Sk\\}$ to dowolny podział obserwacji w $k$ klastrów.\n",
    "\n",
    "Liczymy sumę odległości punktu od środka skupienia (możemy tutaj użyć różnych metryk, my zastosujemy kwadrat odległości $L_2$. Rysujemy wykres tejże odległości w zależności od liczby klastrów i wybieramy punkt „przegięcia”. Wygląda on jak łokieć — stąd nazwa metody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T17:51:00.124799Z",
     "start_time": "2020-04-25T17:51:00.113184Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_wcss_scores(X, k_max):\n",
    "    #  WCSS = within-cluster sum of squares\n",
    "    scores = []\n",
    "    for k in range(1, k_max+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(X)\n",
    "        wcss = kmeans.score(X) * -1 # score returns -WCSS\n",
    "        scores.append(wcss)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T17:51:00.889476Z",
     "start_time": "2020-04-25T17:51:00.516411Z"
    }
   },
   "outputs": [],
   "source": [
    "wcss_vec = count_wcss_scores(X, 10)\n",
    "x_ticks = list(range(1, len(wcss_vec) + 1))\n",
    "plt.plot(x_ticks, wcss_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Within-cluster sum of squares')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metoda łokcia polega na wybraniu takiego *k*, w którym nasz score zaczyna maleć w bardzo powolny sposób.\n",
    "- Które byśmy wybrali na tym wykresie?\n",
    "\n",
    "Oczywiście jest to metoda subiektywna i czasami wybór łokcia jest nieoczywisty. Dlatego często stosuje się inną metodę:\n",
    "\n",
    "#### Metoda silhouette\n",
    "\n",
    "Zdefiniujmy:\n",
    "$$ a(i) := \\frac{1}{ |C(i)|  -1 } \\sum_{j \\in C(i), i \\neq j} d(i, j)$$\n",
    "Gdzie:  \n",
    "$i$ — indeks punktu,  \n",
    "$C(i)$ — klaster, do którego należy $i$-ty punkt,  \n",
    "$d(i, j)$ — odległość między $i$-tym i $j$-tym punktem.\n",
    "\n",
    "Liczbę $a$ możemy interpretować jako średnią odległość od punktu w tym samym klastrze.\n",
    "\n",
    "$$ b(i) := \\underset{k: C_k \\bigcap C(i) = \\emptyset}{min} \\frac{1}{|C_k|} \\sum_{j \\in C_k} d(i, j) $$\n",
    "$C_k$ to $k$-ty klaster. Nie mylić z oznaczeniem $C(i)$, czyli klastrem, do którego należy $i$-ty punkt.\n",
    "\n",
    "\n",
    "Liczba $b$ to po prostu średnia odległość od punktów tego klastra, który jest „drugim najlepszym” dla $i$-tego punktu.  \n",
    "\n",
    "Jeśli $|C(i)| \\neq 1$ definiujemy:  \n",
    "$$ s(i) := \\frac{b(i) - a(i)}{\\max{ \\{ a(i), b(i) \\} }} $$\n",
    "Dla kompletności dodajmy, że jeśli $|C(i)| = 1$ (tylko jedna obserwacja w klastrze), wtedy:\n",
    "$$ s(i) := 0 $$\n",
    "Zatem $s(i)$ mówi nam o tym, jak dobrze dany punkt pasuje do klastra, do którego został przyporządkowany w porównaniu z tym klastrem, który jest „drugim wyborem\".\n",
    "\n",
    "Ostatecznie nasza miara silhouette to średnia $s(i)$ po wszystkich punktach:\n",
    "$$ \\frac{\\sum_{i=1}^{n} s(i)}{n} $$\n",
    "Oczywiście $n$ to liczba wszystkich punktów.\n",
    "\n",
    "Największa wada? Premiowanie \"okrągłych\" klastrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T18:35:10.499816Z",
     "start_time": "2020-04-25T18:35:10.484759Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_clustering_scores(X, cluster_num, model, score_fun):\n",
    "    if isinstance(cluster_num, int):\n",
    "        cluster_num_iter = [cluster_num]\n",
    "    else:\n",
    "        cluster_num_iter = cluster_num\n",
    "        \n",
    "    scores = []    \n",
    "    for k in cluster_num_iter:\n",
    "        model_instance = model(n_clusters=k)\n",
    "        labels = model_instance.fit_predict(X)\n",
    "        wcss = score_fun(X, labels)\n",
    "        scores.append(wcss)\n",
    "    \n",
    "    if isinstance(cluster_num, int):\n",
    "        return scores[0]\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T18:35:11.563010Z",
     "start_time": "2020-04-25T18:35:11.148728Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "cluster_num_seq = range(2, 11) # Niektóre metryki nie działają gdy mamy tylko jeden klaster\n",
    "silhouette_vec = count_clustering_scores(X, cluster_num_seq, KMeans, silhouette_score)\n",
    "plt.plot(cluster_num_seq, silhouette_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inne, podobne metryki:  \n",
    "Indeks Daviesa–Bouldina\n",
    "<details>\n",
    "<summary> Indeks Dunna (dla zainteresowanych — kliknij!) </summary>\n",
    "$$ DI = \\frac{ \\underset{i, j}{\\min} \\delta (C_i, C_j)}{\\underset{k}{\\min} \\Delta (C_k)} $$\n",
    "\n",
    "Gdzie:  \n",
    "$\\delta (C_i, C_j)$ -- odległość między klastrami. Możemy ją liczyć na wiele sposóbów. Są to między innymi:\n",
    "* Średnia odległość między punktami obu klastrów.\n",
    "* Najmniejsza odległość między punktami obu klastrów.\n",
    "* Maksymalna odległość między punktami obu klastrów.\n",
    "* Odległośc między środkami klastrów.\n",
    "\n",
    "$\\Delta (C_k)$ -- wielkość klastra. Znowu mamy wiele pomysłów na jej wyznaczanie:\n",
    "* Maksymalna odległość między punktami.\n",
    "* Średnia odległość między punktami.\n",
    "* Dwukrotność średniej odległości do środka (można o tym myśleć jako o czymś na wzór średnicy).\n",
    "\n",
    "Indeks Dunna \"patrzy\" tylko na największy klaster i najmniejszą odległość między klastrami. Czyli możemy myśleć o tym jako o rozważaniu najgorszego przypadku. Z drugiej strony jeśli odpowiednich $\\delta ()$ i $ \\Delta () $ --  do pewnego stopnia uniezależniamy się od premiowania \"okrągłych\" klastrów.\n",
    "</details>\n",
    "Indeks Calińskiego-Harabasza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metryki interpretowalne\n",
    "Poniższe metryki dobrze sprawdzają się przy doborze liczby klastrów. Z drugiej strony są one trudne w interpretacji. Często mamy pewne oczekiwania wobec klastrów. Na przykład:\n",
    "* Dobra separacja klastrów\n",
    "* Małe odległości wewnątrz klastrów\n",
    "* Klastry podobnej wielkości\n",
    "* Dobra reprezentacja klastrów przez środki\n",
    "* Stabilność klasteryzacji\n",
    "* Duża gęstość klastrów\n",
    "\n",
    "Często metryka, którą wybierzemy, jest związana z tym, czego szukamy. Przykłady metryk:\n",
    "* Dobra separacja klastrów -> minimalna odległość między punktami różnych klastrów\n",
    "* Małe odległości wewnątrz klastrów -> średnia odległość między punktami\n",
    "* Klastry podobnej wielkości -> wariancja wielkości klastrów (na przykład średniej odległości między punktami)\n",
    "* Dobra reprezentacja klastrów przez ich środki -> średnia odległość między punktem w klastrze a środkiem\n",
    "* Stabilność klasteryzacji -> bootstrap i frakcja punktów, które różnią się\n",
    "* Duża gęstość klastrów -> największa odległość między punktami wewnątrz klastra\n",
    "\n",
    "Niestety metryki te są wrażliwe na liczbę klastrów. Na przykład, jeśli naszym celem jest dobra reprezentacja punktów w klastrze przez środki, oczywiście najlepiej będzie, gdy każdy punkt będzie osobnym klastrem. A to chyba nie o to chodziło. Dlatego tego typu metryki najlepiej sprawdzają się, gdy porównujemy różne algorytmy, ale przy ustalonej liczbie klastrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T18:35:14.373756Z",
     "start_time": "2020-04-25T18:35:14.269952Z"
    }
   },
   "outputs": [],
   "source": [
    "# Zaimplementujmy zatem kilka wspomnianych metryk.\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "# def two_class\n",
    "\n",
    "def min_interclust_dist(X, label):\n",
    "    clusters = set(label)\n",
    "    global_min_dist = np.inf\n",
    "    for cluster_i in clusters:\n",
    "        cluster_i_idx = np.where(label == cluster_i)\n",
    "        for cluster_j in clusters:\n",
    "            if cluster_i != cluster_j:\n",
    "                cluster_j_idx = np.where(label == cluster_j)\n",
    "                interclust_min_dist = np.min(distance.cdist(X[cluster_i_idx], X[cluster_j_idx]))\n",
    "                global_min_dist = np.min([global_min_dist, interclust_min_dist])\n",
    "    return global_min_dist\n",
    "\n",
    "def _inclust_mean_dists(X, label):\n",
    "    clusters = set(label)\n",
    "    inclust_dist_list = []\n",
    "    for cluster_i in clusters:\n",
    "        cluster_i_idx = np.where(label == cluster_i)\n",
    "        inclust_dist = np.mean(distance.pdist(X[cluster_i_idx]))\n",
    "        inclust_dist_list.append(inclust_dist)\n",
    "    return inclust_dist_list\n",
    "\n",
    "def mean_inclust_dist(X, label):\n",
    "    inclust_dist_list = _inclust_mean_dists(X, label)\n",
    "    return np.mean(inclust_dist_list)\n",
    "\n",
    "def std_dev_of_inclust_dist(X, label):\n",
    "    inclust_dist_list = _inclust_mean_dists(X, label)\n",
    "    return np.std(inclust_dist_list)\n",
    "\n",
    "def mean_dist_to_center(X, label):\n",
    "    clusters = set(label)\n",
    "    inclust_dist_list = []\n",
    "    for cluster_i in clusters:\n",
    "        cluster_i_idx = np.where(label == cluster_i)\n",
    "        cluster_i_mean = np.mean(X[cluster_i_idx], axis=0, keepdims=True)\n",
    "        inclust_dist = np.mean(distance.cdist(X[cluster_i_idx], cluster_i_mean))\n",
    "        inclust_dist_list.append(inclust_dist)\n",
    "    return np.mean(inclust_dist_list)\n",
    "\n",
    "\n",
    "print(f'Minimal distance between clusters = {count_clustering_scores(X, 4, KMeans, min_interclust_dist):.2f}')\n",
    "print(f'Average distance between points in the same class = '\n",
    "      f'{count_clustering_scores(X, 4, KMeans, mean_inclust_dist):.2f}')\n",
    "print(f'Standard deviation of distance between points in the same class = '\n",
    "      f'{count_clustering_scores(X, 4, KMeans, std_dev_of_inclust_dist):.3f}')\n",
    "print(f'Average distance to cluster center = '\n",
    "      f'{count_clustering_scores(X, 4, KMeans, mean_dist_to_center):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasteryzacja hierarchiczna\n",
    "![](https://46gyn61z4i0t1u1pnq2bbk2e-wpengine.netdna-ssl.com/wp-content/uploads/2018/03/Hierarchical-clustering-2.png)\n",
    "<div align=\"center\">Źródło: https://www.displayr.com/what-is-hierarchical-clustering/</div>\n",
    "\n",
    "Powyżej pokazano koncepcję klasteryzacji aglomeracyjnej, ale na jakiej podstawie wybieramy, które klastry połączyć?  \n",
    "Zasadniczo zawsze wybieramy połączenie tych klastrów, które są najbliżej siebie.\n",
    "Jest kilka pomysłów określania tego, co znaczy „są blisko siebie”. Na przykład:\n",
    "* Połączenie pojedyncze - *minimalna odległość*\n",
    "* Połączenie kompletne - *maksymalna odległość*\n",
    "* Połączenie średnie - *średnia odległość*\n",
    "* Połączenie centroidalne\n",
    "* Połączenie Warda — rozważając połączenie dwóch klastrów, w jaki sposób zmienia ono całkowitą odległość od centroidów. Jak bardzo zmienia się kwadratowa odległość od centroidów. Minimalizujemy ją.\n",
    "![](https://ars.els-cdn.com/content/image/1-s2.0-S2300396016300799-gr6.jpg)\n",
    "<div align=\"center\">Źródło: <em>Tan, P.-N., Steinbach, M., & Kumar, V. (2005). Introduction to data mining. Boston:\n",
    "Addison-Wesley</em></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T18:52:58.414643Z",
     "start_time": "2020-04-25T18:52:58.293309Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters=4, linkage = 'ward')\n",
    "y = model.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co nam jeszcze daje klasteryzacja hierarchiczna: nie musimy znać liczby klastrów. Zauważmy, że model buduje całą hierarchię klastrów. Na przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T18:53:42.982688Z",
     "start_time": "2020-04-25T18:53:42.664625Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# Generujemy nowe dane o mnjejszej liczbie punktów (dla czytelności)\n",
    "X_small, _ = make_blobs(n_samples=20, centers=4, cluster_std=0.9, random_state=314)\n",
    "Z = hierarchy.linkage(X_small, method='average')\n",
    "plt.figure(figsize=(10, 5), dpi= 200, facecolor='w', edgecolor='k')\n",
    "hierarchy.dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co nam to daje?  \n",
    "Dzięki temu możemy nie podawać jawnie liczby klastrów, a powiedzieć na przykład:\n",
    "> Interesują nas klastry, w których odległość między punktami będzie nie większa niż 1,23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T18:55:14.908480Z",
     "start_time": "2020-04-25T18:55:14.781976Z"
    }
   },
   "outputs": [],
   "source": [
    "# A Wtedy możemy zrobić tak:\n",
    "model = AgglomerativeClustering(n_clusters=None, linkage='single', distance_threshold=1.23)\n",
    "y = model.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warto wspomnieć, że bardzo podobna do klasteryzacji aglomeracyjnej jest klasteryzacja deglomeracyjna.\n",
    "\n",
    "W klasteryzacji aglomeracyjnej zaczynamy od pojedynczych punktów i łączymy je kolejno w klastry.\n",
    "W klasteryzacji deglomeracyjnej zaczynamy od jednego „superklastra” zawierającego wszystkie punkty i dzielimy go na mniejsze klastry.\n",
    "\n",
    "Jeśli spojrzymy na drzewo hierarchii, to w metodzie aglomeracyjnej jest ono budowane „od dołu”, w metodzie deglomeracyjnej „od góry”.\n",
    "\n",
    "Metoda deglomeracyjna jest rzadko wykorzystywana ze względu na większą złożoność. Rozważmy szukanie skupień dla $n$ punktów. Już pierwszej iteracji musimy rozważyć $2^n$ możliwych podziałów.\n",
    "Dla porównania metoda aglomeracyjna w pierwszej iteracji ma do przeanalizowania zaledwie $\\frac{n (n-1)}{2}$ możliwych połączeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T18:57:16.514907Z",
     "start_time": "2020-04-25T18:57:16.384999Z"
    }
   },
   "outputs": [],
   "source": [
    "# A Wtedy możemy zrobić tak:\n",
    "model = AgglomerativeClustering(n_clusters=None, linkage='single', distance_threshold=20)\n",
    "y = model.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A co z nieco bardziej wymagającym zbiorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T18:57:19.726125Z",
     "start_time": "2020-04-25T18:57:19.601209Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, _ = make_circles(factor = 0.5, noise=0.08, n_samples=200, random_state=3)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T18:57:22.610475Z",
     "start_time": "2020-04-25T18:57:22.041404Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_agglomerative_clustering(X, n_clusters, linkage):\n",
    "    model = AgglomerativeClustering(n_clusters=2, linkage=linkage)\n",
    "    y = model.fit_predict(X)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='viridis')\n",
    "    plt.title(f'Clusters from agglomerative_clustering with {linkage} linkage')\n",
    "    plt.show()\n",
    "\n",
    "for linkage in {'ward', 'complete', 'average', 'single'}:\n",
    "    plot_agglomerative_clustering(X, n_clusters=2, linkage=linkage)\n",
    "    \n",
    "plot_kmeans_clusters(X, n_clusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T18:57:37.050734Z",
     "start_time": "2020-04-25T18:57:36.751362Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "metrices = {\n",
    "            'min dist between clusters': min_interclust_dist,\n",
    "            'mean dist in clust': mean_inclust_dist,\n",
    "            'std dev dist in clust': std_dev_of_inclust_dist,\n",
    "            'mean dist to clust center': mean_dist_to_center,\n",
    "            'silhouette': silhouette_score\n",
    "           }\n",
    "\n",
    "models = {\n",
    "          'Agglomerative ward linkage': partial(AgglomerativeClustering, linkage='ward'),\n",
    "          'Agglomerative complete linkage': partial(AgglomerativeClustering, linkage='complete'),\n",
    "          'Agglomerative average linkage': partial(AgglomerativeClustering, linkage='average'),\n",
    "          'Agglomerative single linkage': partial(AgglomerativeClustering, linkage='single'),\n",
    "          'Kmeans': KMeans\n",
    "         }\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for model_key in models:\n",
    "    for metric_key in metrices:\n",
    "        df.loc[model_key, metric_key] = count_clustering_scores(X=X, cluster_num=2,\n",
    "                                                                model=models[model_key],\n",
    "                                                                score_fun=metrices[metric_key])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dla zainteresowanych — materiały dodatkowe\n",
    "Badanie porównawcze metryk:  \n",
    "[Bolshakova and Azuaje, 2003](https://www.scss.tcd.ie/publications/tech-reports/reports.02/TCD-CS-2002-33.pdf)  \n",
    "Wykład o weryfikacji jakości klasteryzacji:  \n",
    "[Henning, 2016](https://www.youtube.com/watch?v=Mf6MqIS2ql4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
