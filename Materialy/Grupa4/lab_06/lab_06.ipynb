{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klasteryzacja\n",
    "## Wstęp\n",
    "### Czym jest klasteryzacja?\n",
    "Najprościej — szukaniem skupień (klastrów).\n",
    "\n",
    "![](https://miro.medium.com/max/561/0*ff7kw5DRQbs_uixR.jpg)\n",
    "\n",
    "Żródło: https://www.kdnuggets.com/2019/09/hierarchical-clustering.html\n",
    "\n",
    "### Po co?\n",
    "* Aby znaleźć „naturalne” podziały w zbiorze.\n",
    "* Aby zaproponować podział na klasy.\n",
    "* Aby ułatwić opis (klastrom można przyporządkować etykiety i do pewnego stopnia traktować jako całość).\n",
    "* ...\n",
    "\n",
    "### Czym właściwie jest klaster?\n",
    "Nie mamy jednej definicji. Na ogół — grupa podobnych obiektów. Różne algorytmy rożnie „rozumieją” podobieństwo i różnie go szukają.\n",
    "\n",
    "Zasadniczo metody dzielimy na metody hierarchiczne i kombinatoryczne. Zacznijmy od kombinatorycznych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metoda k-średnich\n",
    "Pomysł jest prosty:\n",
    "\n",
    "1. Zakładamy, że w zbiorze jest k klastrów.\n",
    "2. Wybieramy k punktów będącymi początkowymi położeniami środków naszych klastrów.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/5/5e/K_Means_Example_Step_1.svg)\n",
    "\n",
    "3. Określamy przynależność do klastrów jako przynależność do klastra „generowanego” przez najbliższy środek.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/a/a5/K_Means_Example_Step_2.svg)\n",
    "\n",
    "4. Aktualizujemy położenie środków klastrów jako środek masy punków należących do klastra.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/3/3e/K_Means_Example_Step_3.svg)\n",
    "\n",
    "5. Sprawdzamy, czy przynależność jakiegokolwiek punktu zmieniła się po wyznaczeniu nowych środków. Jeśli tak — wracamy do punktu 3. Jeśli nie — kończymy działanie.\n",
    "\n",
    "Źródło ilustracji: https://en.wikipedia.org/wiki/K-means_clustering\n",
    "\n",
    "Działanie algorytmu wygląda następująco:\n",
    "\n",
    "![](https://miro.medium.com/max/960/1*KrcZK0xYgTa4qFrVr0fO2w.gif)\n",
    "\n",
    "Żródło: https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig_size = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:45.068221Z",
     "start_time": "2020-04-27T21:30:44.528510Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_data(X):\n",
    "    plt.scatter(X[:,0], X[:,1], c='k', s=2)\n",
    "\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.9, random_state=314)\n",
    "f = plt.figure(figsize=fig_size)\n",
    "plot_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:45.762392Z",
     "start_time": "2020-04-27T21:30:45.688465Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_kmeans_centers(model):\n",
    "    centers = model.cluster_centers_\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75)\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c='black', marker='x', s=75, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:46.638608Z",
     "start_time": "2020-04-27T21:30:46.492882Z"
    }
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=fig_size)\n",
    "plot_data(X)\n",
    "plot_kmeans_centers(kmeans)"
   ]
  },
  {
   "source": [
    "### Granice decyzyjne (Voronoi diagram)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_voronoi_diag(model, X, res=1000, margin=0.2):\n",
    "    # source: https://github.com/ageron/handson-ml2/blob/master/09_unsupervised_learning.ipynb\n",
    "    mins = X.min(axis=0) - margin\n",
    "    maxs = X.max(axis=0) + margin\n",
    "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], res),\n",
    "                         np.linspace(mins[1], maxs[1], res))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                cmap=\"Pastel2\")\n",
    "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),\n",
    "                linewidths=1, colors='k')  \n",
    "\n",
    "\n",
    "def plot_kmeans_with_bounds(model, X):\n",
    "    f = plt.figure(figsize=fig_size)\n",
    "    plot_voronoi_diag(kmeans, X)\n",
    "    plot_data(X)\n",
    "    plot_kmeans_centers(kmeans) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kmeans_with_bounds(kmeans, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A co jeśli podamy złe k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:47.910292Z",
     "start_time": "2020-04-27T21:30:47.743725Z"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6, random_state=0)\n",
    "kmeans.fit(X)\n",
    "plot_kmeans_with_bounds(kmeans, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zatem jak dobrać liczbę skupień?\n",
    "\n",
    "#### Metoda łokcia\n",
    "Zacznijmy od tego, że algorytm k-średnich minimalizuję wewnątrzklastrową sumę kwadratów (ang. <em>within-cluster sum of squares</em>):\n",
    "\n",
    "$$\\underset{\\mathbf{S}}{\\operatorname{argmin}}\\sum_{i=1}^{k}\\sum_{\\mathbf{x} \\in S_i} {\\lVert \\mathbf{x} - \\mu_i \\rVert^2} = \\underset{\\mathbf{S}}{\\operatorname{argmin}}\\sum_{i=1}^{k} \\# S_i \\mathrm{Var} (S_i)\n",
    "$$\n",
    "\n",
    "Liczymy sumę odległości punktu od środka skupienia (możemy tutaj użyć różnych metryk, my zastosujemy kwadrat odległości $L_2$, ponieważ jest to naturalna metryka dla naszego algorytmu). Rysujemy wykres tejże odległości w zależności od liczby klastrów i wybieramy punkt „przegięcia”. Wygląda on jak łokieć — stąd nazwa metody."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:48.867386Z",
     "start_time": "2020-04-27T21:30:48.861169Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_wcss_scores(X, k_max):\n",
    "    #  WCSS = within-cluster sum of squares\n",
    "    scores = []\n",
    "    for k in range(1, k_max+1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        kmeans.fit(X)\n",
    "        wcss = kmeans.score(X) * -1 # score returns -WCSS\n",
    "        scores.append(wcss)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:49.898199Z",
     "start_time": "2020-04-27T21:30:49.517281Z"
    }
   },
   "outputs": [],
   "source": [
    "wcss_vec = count_wcss_scores(X, 10)\n",
    "x_ticks = list(range(1, len(wcss_vec) + 1))\n",
    "f = plt.figure(figsize=fig_size)\n",
    "plt.plot(x_ticks, wcss_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Within-cluster sum of squares')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiście jest to metoda subiektywna i czasami wybór łokcia jest nieoczywisty. Dlatego często stosuje się inną metodę:\n",
    "\n",
    "#### Metoda silhouette\n",
    "\n",
    "Zdefiniujmy:\n",
    "$$ a(i) := \\frac{1}{|C(i)|  -1 } \\sum_{j \\in C(i), i \\neq j} d(i, j)$$\n",
    "Gdzie:  \n",
    "$i$ — indeks punktu,  \n",
    "$C(i)$ — klaster, do którego należy $i$-ty punkt,  \n",
    "$d(i, j)$ — odległość między $i$-tym i $j$-tym punktem.\n",
    "\n",
    "Liczbę $a$ możemy interpretować jako średnią odległość od punktu w tym samym klastrze.\n",
    "\n",
    "$$ b(i) := \\underset{k: C_k \\bigcap C(i) = \\emptyset}{min} \\frac{1}{|C_k|} \\sum_{j \\in C_k} d(i, j) $$\n",
    "$C_k$ to $k$-ty klaster. Nie mylić z oznaczeniem $C(i)$, czyli klastrem, do którego należy $i$-ty punkt.\n",
    "\n",
    "\n",
    "Liczba $b$ to po prostu średnia odległość od punktów tego klastra, który jest „drugim najlepszym” dla $i$-tego punktu.  \n",
    "\n",
    "Jeśli $|C(i)| \\neq 1$ definiujemy:  \n",
    "$$ s(i) := \\frac{b(i) - a(i)}{\\max{ \\{ a(i), b(i) \\} }} $$\n",
    "Dla kompletności dodajmy, że jeśli $|C(i)| = 1$, wtedy:\n",
    "$$ s(i) := 0 $$\n",
    "Zatem $s(i)$ mówi nam o tym, jak dobrze dany punkt pasuje do klastra, do którego został przyporządkowany w porównaniu z tym klastrem, który jest „drugim wyborem\".\n",
    "\n",
    "Ostatecznie nasza miara silhouette to średnia $s(i)$ po wszystkich punktach:\n",
    "$$ \\frac{\\sum_{i=1}^{n} s(i)}{n} $$\n",
    "Oczywiście $n$ to liczba wszystkich punktów.\n",
    "\n",
    "Interpretacja $s(i)$:\n",
    "\n",
    "- $s(i) \\simeq \\quad  1$ - próbka i jest blisko centrum skupienia\n",
    "- $s(i) \\simeq \\quad  0$ - próbka i znajduje się w pobliżu granicy \n",
    "- $s(i) \\simeq \\, -1$ - próbka i została (prawdopodobnie) przydzielona do złej grupy\n",
    "\n",
    "Największa wada? Premiowanie \"okrągłych\" klastrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:51.181780Z",
     "start_time": "2020-04-27T21:30:51.173676Z"
    }
   },
   "outputs": [],
   "source": [
    "# A w praktyce wygląda to tak:\n",
    "def count_clustering_scores(X, cluster_num, model_class, score_fun):\n",
    "    # Napiszmy tę funkcje tak ogólnie, jak to możliwe. \n",
    "    # Zwróćcie uwagę na przekazanie obiektów typu callable: model i score_fun.\n",
    "    if isinstance(cluster_num, int):\n",
    "        cluster_num_iter = [cluster_num]\n",
    "    else:\n",
    "        cluster_num_iter = cluster_num\n",
    "        \n",
    "    scores = []    \n",
    "    for k in cluster_num_iter:\n",
    "        model_instance = model_class(n_clusters=k)\n",
    "        labels = model_instance.fit_predict(X)\n",
    "        wcss = score_fun(X, labels)\n",
    "        scores.append(wcss)\n",
    "    \n",
    "    if isinstance(cluster_num, int):\n",
    "        return scores[0]\n",
    "    else:\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:52.206311Z",
     "start_time": "2020-04-27T21:30:51.829241Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "cluster_num_seq = range(2, 11) # Niektóre metryki nie działają gdy mamy tylko jeden klaster\n",
    "silhouette_vec = count_clustering_scores(X, cluster_num_seq, KMeans, silhouette_score)\n",
    "f = plt.figure(figsize=fig_size)\n",
    "plt.plot(cluster_num_seq, silhouette_vec, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Jeszcze więcej informacji uzyskamy rusując współczynnik profilu, a więc wartości $s(i)$ oraz silhouette_score (linia przerywana)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_samples\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "def silhouette_coefficients(model_class, n_clusters, X):\n",
    "    # source: https://github.com/ageron/handson-ml2/blob/master/09_unsupervised_learning.ipynb\n",
    "    f = plt.figure(figsize=fig_size)\n",
    "    y_pred = model_class(n_clusters=n_clusters).fit_predict(X)\n",
    "    silhouette_s = silhouette_score(X, y_pred)\n",
    "    silhouette_coefficients = silhouette_samples(X, y_pred)\n",
    "    padding = len(X) // 30\n",
    "    pos = padding\n",
    "    ticks = []\n",
    "    for i in range(k):\n",
    "            coeffs = silhouette_coefficients[y_pred == i]\n",
    "            coeffs.sort()\n",
    "\n",
    "            color = mpl.cm.inferno(i / k)\n",
    "            plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs,\n",
    "                            facecolor=color, edgecolor=color, alpha=0.7)\n",
    "            ticks.append(pos + len(coeffs) // 2)\n",
    "            pos += len(coeffs) + padding\n",
    "    plt.axvline(x=silhouette_s, linestyle='--', color='r')\n",
    "    plt.title(f'k={n_clusters}', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2,7):\n",
    "    silhouette_coefficients(KMeans, k, X)\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "W praktyce najlepsze rezultaty uzyskujemy gdy w każdej grupie istnieją próbki, których $s(i)$ przekracza średnią (silhouette_score). Jeżeli cała grupa jest wyraźnie poniżej średniej oznacza to, że znajduje się prawdopodobnie zbyt blisko innej grupy (zobacz k=5). \n",
    "\n",
    "Z powyższych przykładów najlepiej prezentują się wykresy k=3 oraz k=4. Ostatecznie decydujemy się na k=4 ze względu na wyrównaną liczność klastrów."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inne metryki:  \n",
    "\n",
    "- Indeks Daviesa–Bouldina\n",
    "\n",
    "- Indeks Dunna \n",
    "\n",
    "$$ DI = \\frac{ \\underset{i, j}{\\min} \\delta (C_i, C_j)}{\\underset{k}{\\max} \\Delta (C_k)} $$\n",
    "\n",
    "Gdzie:  \n",
    "$\\delta (C_i, C_j)$ -- odległość między klastrami. Możemy ją liczyć na wiele sposóbów. Są to między innymi:\n",
    "* Średnia odległość między punktami obu klastrów.\n",
    "* Najmniejsza odległość między punktami obu klastrów.\n",
    "* Maksymalna odległość między punktami obu klastrów.\n",
    "* Odległośc między środkami klastrów.\n",
    "\n",
    "$\\Delta (C_k)$ -- wielkość klastra. Znowu mamy wiele pomysłów na jej wyznaczanie:\n",
    "* Maksymalna odległość między punktami.\n",
    "* Średnia odległość między punktami.\n",
    "* Dwukrotność średniej odległości do środka (można o tym myśleć jako o czymś na wzór średnicy).\n",
    "\n",
    "Indeks Dunna \"patrzy\" tylko na największy klaster i najmniejszą odległość między klastrami. Czyli możemy myśleć o tym jako o rozważaniu najgorszego przypadku. Z drugiej strony jeśli odpowiednich $\\delta ()$ i $ \\Delta () $ --  do pewnego stopnia uniezależniamy się od premiowania \"okrągłych\" klastrów.\n",
    "\n",
    "- Indeks Calińskiego-Harabasza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metryki interpretowalne\n",
    ">*Gdy dowolny wskaźnik zaczyna być traktowany jako cel, przestaje być dobrym wskaźnikiem.*  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Prawo Goodharta\n",
    "\n",
    "Powyższe metryki dobrze sprawdzają się przy doborze liczby klastrów. Z drugiej strony są one trudne w interpretacji. Często mamy pewne oczekiwania wobec klastrów. Na przykład:\n",
    "* Dobra separacja klastrów\n",
    "* Małe odległości wewnątrz klastrów\n",
    "* Klastry podobnej wielkości\n",
    "* Dobra reprezentacja klastrów przez środki\n",
    "* Stabilność klasteryzacji\n",
    "* Duża gęstość klastrów\n",
    "\n",
    "Często metryka, którą wybierzemy, jest związana z tym, czego szukamy. Przykłady metryk:\n",
    "* Dobra separacja klastrów -> minimalna odległość między punktami różnych klastrów\n",
    "* Małe odległości wewnątrz klastrów -> średnia odległość między punktami\n",
    "* Klastry podobnej wielkości -> wariancja wielkości klastrów (na przykład średniej odległości między punktami)\n",
    "* Dobra reprezentacja klastrów przez ich środki -> średnia odległość między punktem w klastrze a środkiem\n",
    "* Stabilność klasteryzacji -> bootstrap i frakcja punktów, które różnią się\n",
    "* Duża gęstość klastrów -> największa odległość między punktami wewnątrz klastra\n",
    "\n",
    "Niestety metryki te są wrażliwe na liczbę klastrów. Na przykład, jeśli naszym celem jest dobra reprezentacja punktów w klastrze przez środki, oczywiście najlepiej będzie, gdy każdy punkt będzie osobnym klastrem. A to chyba nie o to chodziło. Dlatego tego typu metryki najlepiej sprawdzają się, gdy porównujemy różne algorytmy, ale przy ustalonej liczbie klastrów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:54.164816Z",
     "start_time": "2020-04-27T21:30:54.070846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Zaimplementujmy zatem kilka wspomnianych metryk.\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def min_interclust_dist(X, label):\n",
    "    clusters = set(label)\n",
    "    global_min_dist = np.inf\n",
    "    for cluster_i in clusters:\n",
    "        cluster_i_idx = np.where(label == cluster_i)\n",
    "        for cluster_j in clusters:\n",
    "            if cluster_i != cluster_j:\n",
    "                cluster_j_idx = np.where(label == cluster_j)\n",
    "                interclust_min_dist = np.min(distance.cdist(X[cluster_i_idx], X[cluster_j_idx]))\n",
    "                global_min_dist = np.min([global_min_dist, interclust_min_dist])\n",
    "    return global_min_dist\n",
    "\n",
    "def _inclust_mean_dists(X, label):\n",
    "    clusters = set(label)\n",
    "    inclust_dist_list = []\n",
    "    for cluster_i in clusters:\n",
    "        cluster_i_idx = np.where(label == cluster_i)\n",
    "        inclust_dist = np.mean(distance.pdist(X[cluster_i_idx]))\n",
    "        inclust_dist_list.append(inclust_dist)\n",
    "    return inclust_dist_list\n",
    "\n",
    "def mean_inclust_dist(X, label):\n",
    "    inclust_dist_list = _inclust_mean_dists(X, label)\n",
    "    return np.mean(inclust_dist_list)\n",
    "\n",
    "def std_dev_of_inclust_dist(X, label):\n",
    "    inclust_dist_list = _inclust_mean_dists(X, label)\n",
    "    return np.std(inclust_dist_list)\n",
    "\n",
    "def mean_dist_to_center(X, label):\n",
    "    clusters = set(label)\n",
    "    inclust_dist_list = []\n",
    "    for cluster_i in clusters:\n",
    "        cluster_i_idx = np.where(label == cluster_i)\n",
    "        cluster_i_mean = np.mean(X[cluster_i_idx], axis=0, keepdims=True)\n",
    "        inclust_dist = np.mean(distance.cdist(X[cluster_i_idx], cluster_i_mean))\n",
    "        inclust_dist_list.append(inclust_dist)\n",
    "    return np.mean(inclust_dist_list)\n",
    "\n",
    "\n",
    "print(f'Minimal distance between clusters = {count_clustering_scores(X, 4, KMeans, min_interclust_dist):.2f}.')\n",
    "print(f'Average distance between points in the same class = '\n",
    "      f'{count_clustering_scores(X, 4, KMeans, mean_inclust_dist):.2f}.')\n",
    "print(f'Standard deviation of distance between points in the same class = '\n",
    "      f'{count_clustering_scores(X, 4, KMeans, std_dev_of_inclust_dist):.3f}.')\n",
    "print(f'Average distance to cluster center = '\n",
    "      f'{count_clustering_scores(X, 4, KMeans, mean_dist_to_center):.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasteryzacja hierarchiczna\n",
    "\n",
    "![](https://46gyn61z4i0t1u1pnq2bbk2e-wpengine.netdna-ssl.com/wp-content/uploads/2018/03/Hierarchical-clustering-2.png)\n",
    "\n",
    "Źródło: https://www.displayr.com/what-is-hierarchical-clustering/\n",
    "\n",
    "Powyżej pokazano koncepcję klasteryzacji aglomeracyjnej, ale na jakiej podstawie wybieramy, które klastry połączyć?  \n",
    "Zasadniczo zawsze wybieramy połączenie tych klastrów, które są najbliżej siebie.\n",
    "Jest kilka pomysłów określania tego, co znaczy „są blisko siebie”. Na przykład:\n",
    "* Połączenie kompletne\n",
    "* Połączenie pojedyncze\n",
    "* Połączenie średnie\n",
    "* Połączenie centroidalne\n",
    "* Połączenie Warda — o ile wzrośnie wariancja nowego klastra względem sumy wariancji starych klastrów\n",
    "\n",
    "![](https://ars.els-cdn.com/content/image/1-s2.0-S2300396016300799-gr6.jpg)\n",
    "\n",
    "Źródło: Tan, P.-N., Steinbach, M., & Kumar, V. (2005). Introduction to data mining. Boston:\n",
    "Addison-Wesley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:56.348812Z",
     "start_time": "2020-04-27T21:30:56.229396Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters=4)\n",
    "y_pred = model.fit_predict(X)\n",
    "f = plt.figure(figsize=fig_size)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=30, cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co nam jeszcze daje klasteryzacja hierarchiczna: nie musimy znać liczby klastrów. Zauważmy, że model buduje całą hierarchię klastrów. Na przykład:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:57.683948Z",
     "start_time": "2020-04-27T21:30:57.326370Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "\n",
    "# Generujemy nowe dane o mnjejszej liczbie punktów (dla czytelności)\n",
    "X_small, _ = make_blobs(n_samples=20, centers=4, cluster_std=0.9, random_state=314)\n",
    "Z = hierarchy.linkage(X_small, method='average')\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.scatter(X_small[:,0], X_small[:, 1], s=30, c='k')\n",
    "plt.show()\n",
    "plt.figure(figsize=(16, 8), facecolor='w', edgecolor='k')\n",
    "hierarchy.dendrogram(Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co nam to daje?  \n",
    "Dzięki temu możemy nie podawać jawnie liczby klastrów, a powiedzieć na przykład:\n",
    "> Interesują nas klastry, w których odległość między punktami będzie nie większa niż 6.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:30:58.929320Z",
     "start_time": "2020-04-27T21:30:58.813137Z"
    }
   },
   "outputs": [],
   "source": [
    "# A Wtedy możemy zrobić tak:\n",
    "model = AgglomerativeClustering(n_clusters=None, linkage='complete', distance_threshold=6.25)\n",
    "y_pred = model.fit_predict(X)\n",
    "f = plt.figure(figsize=fig_size)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, s=30, cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warto wspomnieć, że bardzo podobna do klasteryzacji aglomeracyjnej jest klasteryzacja deglomeracyjna.\n",
    "\n",
    "W klasteryzacji aglomeracyjnej zaczynamy od pojedynczych punktów i łączymy je kolejno w klastry.\n",
    "W klasteryzacji deglomeracyjnej zaczynamy od jednego „superklastra” zawierającego wszystkie punkty i dzielimy go na mniejsze klastry.\n",
    "\n",
    "Jeśli spojrzymy na drzewo hierarchii, to w metodzie aglomeracyjnej jest ono budowane „od dołu”, w metodzie deglomeracyjnej „od góry”.\n",
    "\n",
    "Metoda deglomeracyjna jest rzadko wykorzystywana ze względu na większą złożoność. Rozważmy szukanie skupień dla $n$ punktów. Już pierwszej iteracji musimy rozważyć $2^n$ możliwych podziałów.\n",
    "Dla porównania metoda aglomeracyjna w pierwszej iteracji ma do przeanalizowania zaledwie $\\frac{n (n-1)}{2}$ możliwych połączeń."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A co z nieco bardziej wymagającym zbiorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:31:01.111119Z",
     "start_time": "2020-04-27T21:31:01.005504Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, _ = make_circles(factor = 0.5, noise=0.08, n_samples=200, random_state=3)\n",
    "f = plt.figure(figsize=fig_size)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:31:02.261395Z",
     "start_time": "2020-04-27T21:31:01.661690Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_agglomerative_clustering(X, n_clusters, linkage):\n",
    "    model = AgglomerativeClustering(n_clusters=2, linkage=linkage)\n",
    "    y = model.fit_predict(X)\n",
    "\n",
    "    f = plt.figure(figsize=fig_size)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap='brg')\n",
    "    plt.title(f'Clusters from agglomerative_clustering with {linkage} linkage')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for linkage in {'ward', 'complete', 'average', 'single'}:\n",
    "    plot_agglomerative_clustering(X, n_clusters=2, linkage=linkage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans.fit(X)\n",
    "plot_kmeans_with_bounds(kmeans, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T21:31:02.746793Z",
     "start_time": "2020-04-27T21:31:02.297330Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "metrices = {\n",
    "            'min dist between clusters': min_interclust_dist,\n",
    "            'mean dist in clust': mean_inclust_dist,\n",
    "            'std dev dist in clust': std_dev_of_inclust_dist,\n",
    "            'mean dist to clust center': mean_dist_to_center,\n",
    "            'silhouette': silhouette_score\n",
    "           }\n",
    "\n",
    "models = {\n",
    "          'Agglomerative ward linkage': partial(AgglomerativeClustering, linkage='ward'),\n",
    "          'Agglomerative complete linkage': partial(AgglomerativeClustering, linkage='complete'),\n",
    "          'Agglomerative average linkage': partial(AgglomerativeClustering, linkage='average'),\n",
    "          'Agglomerative single linkage': partial(AgglomerativeClustering, linkage='single'),\n",
    "          'Kmeans': KMeans\n",
    "         }\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for model_key in models:\n",
    "    for metric_key in metrices:\n",
    "        df.loc[model_key, metric_key] = count_clustering_scores(X=X, cluster_num=2,\n",
    "                                                                model_class=models[model_key],\n",
    "                                                                score_fun=metrices[metric_key])\n",
    "df"
   ]
  },
  {
   "source": [
    "## BONUS\n",
    "\n",
    "![](https://preview.redd.it/5zz3wrn5ypm41.jpg?auto=webp&s=c587ea97c2c9998ac2ec7ad8effdaed76f628b08)\n",
    "\n",
    "### Segmentacja obrazu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.image import imread\n",
    "image = imread('plane.jpg')\n",
    "f = plt.figure(figsize=fig_size)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_segmentation(model, image):\n",
    "    X = image.reshape(-1, 3)\n",
    "    model.fit(X)\n",
    "    segmented_img = model.cluster_centers_[model.labels_] / 255\n",
    "    return segmented_img.reshape(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(2,5):\n",
    "    segmented_img = img_segmentation(KMeans(n_clusters=k), image)\n",
    "    f = plt.figure(figsize=fig_size)\n",
    "    plt.imshow(segmented_img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'k={k}')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dla zainteresowanych — materiały dodatkowe\n",
    "Badanie porównawcze metryk:  \n",
    "[Bolshakova and Azuaje, 2003](https://www.scss.tcd.ie/publications/tech-reports/reports.02/TCD-CS-2002-33.pdf)  \n",
    "Wykład o weryfikacji jakości klasteryzacji:  \n",
    "[Henning, 2016](https://www.youtube.com/watch?v=Mf6MqIS2ql4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python386jvsc74a57bd0adec68d5d8f86c04e00bdfcc272eb9a1a320647350f49cbe1ec4dd83b70c43c6",
   "display_name": "Python 3.8.6 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}