{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "lab_09.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ifx29Ud5Vis"
      },
      "source": [
        "# Wstęp do sztucznych sieci neuronowych\n",
        "### Czym są sieci neuronowe?\n",
        "Najprościej -- uniwersalnym aproksymatorem funkcji. To, co jest ważne, to że będziemy uczyć sieć neuronową aproksymować pewną (wybraną przez nas) funkcję.\n",
        "\n",
        "\n",
        "### A w praktyce?\n",
        "W praktyce sieci neuronowe prezentowane są jako złożenie kombinacji liniowych i nieliniowych przekształceń. Brzmi skomplikowanie, dlatego zacznijmy bardzo prostego przykładu, sieci z jednym neuronem:\n",
        "\n",
        "\n",
        "![](https://www.researchgate.net/publication/286020106/figure/fig13/AS:328309167673363@1455286414002/Basic-artificial-neural-network-cell-artificial-neuron.png)\n",
        "Źródło: Hüseyin Ceylan, An Artificial Neural Networks Approach to Estimate\n",
        "Occupational Accident: A National Perspective for Turkey, 2014\n",
        "\n",
        "W prakyce sieci neuronowe wizualizowane są przez tego typu obrazki:\n",
        "![](https://visualstudiomagazine.com/articles/2014/11/01/~/media/ECG/visualstudiomagazine/Images/2014/11/1114vsm_mccaffreyfig2.ashx)\n",
        "Źródło: https://visualstudiomagazine.com/articles/2014/11/01/use-python-with-your-neural-networks.aspx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA80uCnw5Viu"
      },
      "source": [
        "Spójrzmy teraz na sieć w nieco inny sposób niż na powyższych obrazkach. Pomyślmy, jak opisać ją wzorem. Z pominięciem składowej stałej możemy powyższą sieć opisać wzorem:\n",
        "\n",
        "$$ y = W_2 \\cdot f(W_1 \\cdot x) $$\n",
        "\n",
        "Gdzie:  \n",
        "$ y $  -- wektor wyjść  \n",
        "$ x $  -- wektor wejść  \n",
        "$ f() $ -- nieliniowa funkcja $ \\mathbb{R}^n \\to \\mathbb{R}^n $  \n",
        "$ W_1 $, $ W_2 $ -- macierze wag odpowiednich warstw\n",
        "\n",
        "Dlaczego ważny jest zapis macierzowy? Komputer bardzo dobrze zrównolegla obliczenia macierzowe. W szczególności karty graficzne specjalizują się w obliczeniach macierzowych. Dlatego trening sieci na karcie graficznej jest zazwyczaj kilkadziesiąt razy szybszy niż na procesorze."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yQyuIrE5Viu"
      },
      "source": [
        "### Jak wytrenować sieć neuronową?\n",
        "Do tej pory mówiliśmy o tym, jak sieci neuronowe wyznaczają wyjście dla zadanego wejścia. Ale jak je nauczyć? Odpowiedzią jest algorytm wstecznej propagacji gradientu. Na wysokim poziomie działa to tak:\n",
        "* Dajemy sieci wejście i mówimy: zgadnij, jaka jest odpowiedź\n",
        "![](https://hmkcode.com/images/ai/bp_forward.png)\n",
        "* Sieć zwraca nam wyjście, a my liczymy ile się pomyliła\n",
        "![](https://hmkcode.com/images/ai/bp_error.png)\n",
        "* Wyznaczamy różniczki błędu po poszczególnych wagach. Liczymy to w dość sprytny sposób, ale nie będziemy się w to teraz wgłębiać\n",
        "* Znając tę różniczki, możemy powiedzieć jak mocno poszczególne wagi wpłynęły na uzyskany błąd. Dlatego aktualizujemy wagi według wzoru:\n",
        "$$ W_{k, i, j} = W_{k, i, j} - a \\cdot {{\\partial E}\\over{\\partial W_{k, i, j}}}$$\n",
        "Gdzie:  \n",
        "$W_{k, i, j}$ -- waga połączenia z $i$-tego neuronu $k-1$ warstwy do $j$-tego neuronu $k$-tej warstwy  \n",
        "$E$ -- obliczona \"różnica\" między wartością zwróconą a tą, której się spodziewaliśmy  \n",
        "$a$ -- stała uczenia\n",
        "\n",
        "W rzeczywistości takie uczenie byłoby bardzo niestabilne, ponieważ każdy punkt ze zbioru uczącego \"ciągnąłby\" parametry w swoją stronę. Dlatego uczymy w tak zwanych mini-batchach składających się z kilkunastu -- kilkuset punktów jednocześnie. Pochodne obliczone dla każdego z punktów z batcha dodajemy do siebie i otrzymujemy \"średni kierunek poprawiający\". W tej sposób sieć uczy się znacznie bardziej stabilnie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WafhkozW5Viv"
      },
      "source": [
        "### A jak to zaprogramować?\n",
        "* Można napisać wszystko ręcznie -- zapewne czeka Was jeszcze taki projekt.\n",
        "* Można skorzystać z dedykowanych bibliotek, na przykład `pytorch`, `tensorflow` czy `keras` \n",
        "* Można skorzystać z `sklearn.neural_network`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SzKjSRE5Viw"
      },
      "source": [
        "### Przykład 1. -- klasyfikacja\n",
        "W związku z tym, że sieć neuronowa może mieć wiele wyjść, bardzo naturalne jest jej wykorzystanie do zadania klasyfikacji wieloklasowej. Pomysł jest taki, żeby $i$-te wyjście mówiło o prawdopodobieństwie przynależności do $i$-tej klasy. Aby to osiągnąć, normalizuje się wyjścia funkcją sofmax:\n",
        "$$ {{e^{y_i}}\\over {\\sum_{j=1}^K e^{y_j}}} $$\n",
        "\n",
        "Gdzie:  \n",
        "$y_i$ -- $i$-te wyjście  \n",
        "$K$ -- liczba klas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IboE2mNN5Vix"
      },
      "source": [
        "Zbiór mnist zawiera ręcznie pisane cyfry.\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
        "Żródło: https://en.wikipedia.org/wiki/MNIST_database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HZsL6-35Viy"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.activations import *\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye9X8onuErEF"
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
        "plt.imshow(X_train_full[0], cmap='binary')\n",
        "y_train_full.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH6c4IjUEsko"
      },
      "source": [
        "X_train, y_train = X_train_full[:50_000] / 255., y_train_full[:50_000]\n",
        "X_valid, y_valid = X_train_full[50_000:] / 255., y_train_full[50_000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZW75Aq_FTMX"
      },
      "source": [
        "Do stworzenia modelu wykorzystamy model sekwencyjny. W ten sposób dane będą przetwarzane po kolei przez wszystkie warstwy, które dodamy. \n",
        "\n",
        "Poza interfejsem sekwencyjnym w bibliotece Tensorflow występuje także interfejs funkcjonalny oraz podklasowy, ale o nich dzisiaj nie będzie.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxnmE6LDE2Fg"
      },
      "source": [
        "def build_model(hidden_layer_size):\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=(28,28)))\n",
        "    model.add(Dense(hidden_layer_size, activation='relu'))\n",
        "    model.add(Dense(hidden_layer_size, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-lkv1-RFEWR"
      },
      "source": [
        "Tensorflow wymaga kompilacji modelu. Przy okazji musimy określić funkcję błędu, optymalizator oraz metryki służące do oceny modelu. W naszym przypadku wykorzystamy optymalizator Stochastic Gradient Descent (SGD). Musimy także podać krok uczenia - learninig rate. **Jest to najważniejszy hiperparametr przy uczeniu modeli głębokich.** Najwięcej czasu powinno być poświęcone właśnie na jego poprawne dobranie. W ogólności nie musi on być stały, ale zmieniać się np wraz z numerem iteracji fazy uczenia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDNAClapE2J8"
      },
      "source": [
        "model = build_model(256)\n",
        "lr = 2e-1\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "             optimizer=SGD(learning_rate=lr),\n",
        "             metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ha2PG34E9Vg"
      },
      "source": [
        "h = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3TINZYpL5p1"
      },
      "source": [
        "plt.plot(h.history['loss'], label='loss')\n",
        "plt.plot(h.history['val_loss'], label='val_loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEAJRMkOVqGk"
      },
      "source": [
        "plt.plot(h.history['accuracy'], label='accuracy')\n",
        "plt.plot(h.history['val_accuracy'], label='val_accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGsS9ftVTyML"
      },
      "source": [
        "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
        "\n",
        "conf_m = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "row_sums = conf_m.sum(axis=1)\n",
        "conf_m = conf_m / row_sums[:, np.newaxis]\n",
        "\n",
        "mask = np.ones(conf_m.shape, dtype=bool)\n",
        "np.fill_diagonal(mask, 0)\n",
        "max_value = conf_m[mask].max()\n",
        "\n",
        "plt.figure(figsize = (15,12))\n",
        "sns.heatmap(conf_m, annot=True, vmax=max_value, cmap=\"YlGnBu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpaeo-1IELLE"
      },
      "source": [
        "## Przykład 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN3XHhaxLkQ2"
      },
      "source": [
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ8_qPUkEupS"
      },
      "source": [
        "tensorboard_path = os.path.join(os.curdir, 'my_logs')\n",
        "def get_run_logdir():\n",
        "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
        "    return os.path.join(tensorboard_path, run_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt9m2ebmL1EQ"
      },
      "source": [
        "model = Sequential([\n",
        "    Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=tf.keras.optimizers.SGD(learning_rate=1e-3))\n",
        "\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir())\n",
        "earlystop_cb = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "h = model.fit(X_train, y_train, epochs=50, validation_data=(X_valid, y_valid), \n",
        "          callbacks=[tensorboard_cb, earlystop_cb])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONEhxtMDL59s"
      },
      "source": [
        "mse_test = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNraDXCnE_d-"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./my_logs --port=6006"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab34d5m-5Vi8"
      },
      "source": [
        "### Kiedy używać sieci neuronowych:\n",
        "\n",
        "1. Kiedy mamy duży zbiór danych\n",
        "2. W zadaniach podobnych do tych, w których ktoś już z powodzeniem użył sieci neuronowych\n",
        "3. Jeśli istnieją architektury dostosowane do naszego problemu\n",
        "4. Kiedy nie zależy nam na interpretowalności\n",
        "5. Gdy mamy dostępną dużą moc obliczeniową do treningu\n",
        "6. Kiedy mamy dużo czasu na przygotowanie modelu\n",
        "7. Gdy rozwiązanie na produkcji nie musi działać bardzo szybko lub jesteśmy w stanie zapewnić dużą moc obliczeniowa również na produkcję\n",
        "8. Gdy zależy nam na (pewnej) odporności na szum na wejściu\n",
        "9. Jeśli chcemy \"wyciągnąć\" reprezentację z warstwy ukrytej\n",
        "10. Jeśli zależy nam na pewnej elastyczności wynikającej z modularnej budowy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9IcyAddCTx4"
      },
      "source": [
        "## Ćwiczenie \n",
        "Klasyfikacja obrazów - Cifar-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZObnfmoOS3hd"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "plt.imshow(X_train[0], cmap='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcHAnnkHS-fj"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=5_000)\n",
        "for s in [X_train, y_train, X_val, y_val, X_test, y_test]:\n",
        "    print(s.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}