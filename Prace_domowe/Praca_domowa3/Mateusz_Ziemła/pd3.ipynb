{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tensorflow",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonpyarray = lambda x: np.asarray(x).astype('float32')\n",
    "data = pd.read_csv('australia.csv').sample(frac=1)\n",
    "\n",
    "train_test_percentage = 0.8\n",
    "rows = len(data.index)\n",
    "# Split train/eval\n",
    "dftrain = data.head(int(rows * train_test_percentage))\n",
    "dfeval = data.tail(int(rows * (1 - train_test_percentage)))"
   ]
  },
  {
   "source": [
    "# Boosted trees"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpdmr9rjlm\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpdmr9rjlm', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpdmr9rjlm/model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 0.69314724, step = 0\n",
      "INFO:tensorflow:global_step/sec: 19.8083\n",
      "INFO:tensorflow:loss = 0.42774647, step = 99 (5.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 19.5981\n",
      "INFO:tensorflow:loss = 0.40812168, step = 199 (5.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 16.9608\n",
      "INFO:tensorflow:loss = 0.34410068, step = 299 (5.895 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.0095\n",
      "INFO:tensorflow:loss = 0.3530105, step = 399 (7.138 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.7764\n",
      "INFO:tensorflow:loss = 0.34647545, step = 499 (8.492 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5656\n",
      "INFO:tensorflow:loss = 0.32957858, step = 599 (9.466 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 603...\n",
      "INFO:tensorflow:Saving checkpoints for 603 into /tmp/tmpdmr9rjlm/model.ckpt.\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 603...\n",
      "INFO:tensorflow:Loss for final step: 0.34307218.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-04-12T17:47:35Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpdmr9rjlm/model.ckpt-603\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 0.54282s\n",
      "INFO:tensorflow:Finished evaluation at 2021-04-12-17:47:36\n",
      "INFO:tensorflow:Saving dict for global step 603: accuracy = 0.8582824, accuracy_baseline = 0.77904814, auc = 0.8863201, auc_precision_recall = 0.73276937, average_loss = 0.33937067, global_step = 603, label/mean = 0.22095187, loss = 0.337703, precision = 0.7902597, prediction/mean = 0.22130619, recall = 0.48816687\n",
      "WARNING:tensorflow:Issue encountered when serializing resources.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'_Resource' object has no attribute 'name'\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 603: /tmp/tmpdmr9rjlm/model.ckpt-603\n",
      "{'accuracy': 0.8582824, 'accuracy_baseline': 0.77904814, 'auc': 0.8863201, 'auc_precision_recall': 0.73276937, 'average_loss': 0.33937067, 'label/mean': 0.22095187, 'loss': 0.337703, 'precision': 0.7902597, 'prediction/mean': 0.22130619, 'recall': 0.48816687, 'global_step': 603}\n"
     ]
    }
   ],
   "source": [
    "fc = tf.feature_column\n",
    "\n",
    "\n",
    "\n",
    "def make_input_fn(X, y, n_epochs=None, shuffle=True):\n",
    "  def input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X.to_dict(orient='list'), y))\n",
    "    if shuffle:\n",
    "      dataset = dataset.shuffle(2534)\n",
    "    dataset = (dataset\n",
    "      .repeat(n_epochs)\n",
    "      .batch(2534))\n",
    "    return dataset\n",
    "  return input_fn\n",
    "\n",
    "def one_hot_cat_column(feature_name, vocab):\n",
    "    return fc.indicator_column(\n",
    "    fc.categorical_column_with_vocabulary_list(feature_name,\n",
    "                                                vocab))\n",
    "\n",
    "y_train = dftrain.pop('RainTomorrow')\n",
    "y_eval = dfeval.pop('RainTomorrow')\n",
    "y_train = tonpyarray(y_train)\n",
    "y_eval = tonpyarray(y_eval)\n",
    "\n",
    "params = {\n",
    "    'n_trees': 100,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate':5e-3,\n",
    "    'n_batches_per_layer': 1,\n",
    "    'center_bias': True\n",
    "}\n",
    "\n",
    "feature_columns = []\n",
    "for feature_name in data.columns[:-1]:\n",
    "    feature_columns.append(fc.numeric_column(feature_name,\n",
    "                                            dtype=tf.float32))\n",
    "\n",
    "est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\n",
    "# Train model.\n",
    "train_input_fn = make_input_fn(dftrain, y_train)\n",
    "est.train(train_input_fn, max_steps=1000)\n",
    "\n",
    "# # Evaluation.\n",
    "eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)\n",
    "trees_result = est.evaluate(eval_input_fn)\n",
    "print(trees_result)"
   ]
  },
  {
   "source": [
    "# K neighbors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='kd_tree', leaf_size=40, n_jobs=-1,\n",
       "                     n_neighbors=64, p=1.5, weights='distance')"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "knn = KNNClassifier(\n",
    "    n_neighbors=64,\n",
    "    weights='distance',\n",
    "    algorithm='kd_tree',\n",
    "    leaf_size = 40,\n",
    "    p=1.5,\n",
    "    n_jobs = -1\n",
    ")\n",
    "x_train = tonpyarray(dftrain[data.columns[:-1]])\n",
    "y_train = tonpyarray(dftrain[data.columns[-1]])\n",
    "knn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_eval = tonpyarray(dfeval[data.columns[:-1]])\n",
    "y_eval = tonpyarray(dfeval[data.columns[-1]])\n",
    "y_pred = knn.predict(x_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.sum((y_eval==1) &  (y_pred==1))\n",
    "tn = np.sum((y_eval==0) &  (y_pred==0))\n",
    "fp = np.sum((y_eval==0) &  (y_pred==1))\n",
    "fn = np.sum((y_eval==1) &  (y_pred==0))\n",
    "\n",
    "knn_result = {\n",
    "    'accuracy' : (tp+tn)/(tp+tn+fp+fn),\n",
    "    'recall' :  (tp)/(tp+fp),\n",
    "    'precision' : (tp)/(tp+fn)\n",
    "}\n"
   ]
  },
  {
   "source": [
    "# Small neural net\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(45136, 17)"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "y_train_o[0]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "1411/1411 [==============================] - 5s 3ms/step - loss: 12.6697 - accuracy: 0.7374 - recall: 0.7374 - precision: 0.7374 - val_loss: 0.4315 - val_accuracy: 0.8230 - val_recall: 0.8230 - val_precision: 0.8230\n",
      "Epoch 2/10\n",
      "1411/1411 [==============================] - 4s 3ms/step - loss: 0.4192 - accuracy: 0.8123 - recall: 0.8123 - precision: 0.8123 - val_loss: 0.3662 - val_accuracy: 0.8328 - val_recall: 0.8328 - val_precision: 0.8328\n",
      "Epoch 3/10\n",
      "1411/1411 [==============================] - 4s 3ms/step - loss: 0.4058 - accuracy: 0.8237 - recall: 0.8237 - precision: 0.8237 - val_loss: 0.3638 - val_accuracy: 0.8366 - val_recall: 0.8366 - val_precision: 0.8366\n",
      "Epoch 4/10\n",
      "1411/1411 [==============================] - 4s 3ms/step - loss: 0.4017 - accuracy: 0.8279 - recall: 0.8279 - precision: 0.8279 - val_loss: 0.3646 - val_accuracy: 0.8328 - val_recall: 0.8328 - val_precision: 0.8328\n",
      "Epoch 5/10\n",
      "1411/1411 [==============================] - 4s 3ms/step - loss: 0.3945 - accuracy: 0.8286 - recall: 0.8286 - precision: 0.8286 - val_loss: 0.3941 - val_accuracy: 0.8381 - val_recall: 0.8381 - val_precision: 0.8381\n",
      "Epoch 6/10\n",
      "1411/1411 [==============================] - 4s 3ms/step - loss: 0.3942 - accuracy: 0.8321 - recall: 0.8321 - precision: 0.8321 - val_loss: 0.3640 - val_accuracy: 0.8412 - val_recall: 0.8412 - val_precision: 0.8412\n",
      "Epoch 7/10\n",
      "1411/1411 [==============================] - 4s 3ms/step - loss: 0.3963 - accuracy: 0.8320 - recall: 0.8320 - precision: 0.8320 - val_loss: 0.3495 - val_accuracy: 0.8477 - val_recall: 0.8477 - val_precision: 0.8477\n",
      "Epoch 8/10\n",
      "1411/1411 [==============================] - 4s 3ms/step - loss: 0.3919 - accuracy: 0.8304 - recall: 0.8304 - precision: 0.8304 - val_loss: 0.3636 - val_accuracy: 0.8505 - val_recall: 0.8505 - val_precision: 0.8505\n",
      "Epoch 9/10\n",
      "1411/1411 [==============================] - 4s 3ms/step - loss: 0.3892 - accuracy: 0.8362 - recall: 0.8362 - precision: 0.8362 - val_loss: 0.3567 - val_accuracy: 0.8441 - val_recall: 0.8441 - val_precision: 0.8441\n",
      "Epoch 10/10\n",
      "1411/1411 [==============================] - 4s 3ms/step - loss: 0.3945 - accuracy: 0.8319 - recall: 0.8319 - precision: 0.8319 - val_loss: 0.3558 - val_accuracy: 0.8463 - val_recall: 0.8463 - val_precision: 0.8463\n"
     ]
    }
   ],
   "source": [
    "onehot = lambda arr : np.array([np.array([1,0]) if v == 1 else np.array([0,1]) for v in arr])\n",
    "neural = keras.Sequential([\n",
    "    keras.layers.Dense(64,activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(64,activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(2,activation='softmax') \n",
    "])\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "neural.compile(optimizer = optimizer , loss = \"binary_crossentropy\", metrics=[\"accuracy\",\"Recall\",\"Precision\"])\n",
    "\n",
    "y_train_o = onehot(y_train)\n",
    "y_eval_o = onehot(y_eval)\n",
    "\n",
    "neural_history = neural.fit(x_train,y_train_o,epochs=10,validation_data=(x_eval,y_eval_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_result = {\n",
    "    'accuracy' : neural_history.history['accuracy'][-1],\n",
    "    'recall' :  neural_history.history['recall'][-1],\n",
    "    'precision' : neural_history.history['precision'][-1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Boosted trees:\n    accuracy: 0.8583\n    recall: 0.4882\n    precision: 0.7903\nK neighbors:\n    accuracy: 0.8559\n    recall: 0.7802\n    precision: 0.4878\nNeural network:\n    accuracy: 0.8333\n    recall: 0.8333\n    precision: 0.8333\n"
     ]
    }
   ],
   "source": [
    "space = '    '\n",
    "for name, d in zip(['Boosted trees','K neighbors', 'Neural network'],[trees_result,knn_result,neural_result]):\n",
    "    print(name + ':')\n",
    "    for k in ['accuracy','recall','precision']:\n",
    "        print(space + f'{k}: {d[k]:.4f}')\n"
   ]
  },
  {
   "source": [
    "# Comparison\n",
    "\n",
    "Boosted trees, despite having the highest accuracy, have a small recall rate, probably due to being made out of weak learners, which have a harder time remembering datapoints compared to other models.\n",
    "\n",
    "K neighbors have a much higher recall than precision, which might be due to recognizing similar datapoints with much ease using plain old distance.\n",
    "\n",
    "The neural network has around equal accuracy, recall and precision, signifying that FP ~= FN, which means that it is biased - as P != N in the set. \n",
    "\n",
    "Boosted trees are probably the best choice for an unbiased, quickly learning model of this calibre, as they scale much better than K neighbors, and are simpler and perform relatively better than the neural network."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}